"""
RunPod ì„œë²„ë¦¬ìŠ¤ LoRA í•™ìŠµ ì›Œì»¤
"""

import runpod
import torch
import os
import zipfile
import requests
from pathlib import Path

# Kohya í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ í™œìš©
def train_lora_kohya(config):
    """Kohya ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•œ LoRA í•™ìŠµ"""
    
    # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    train_script = f"""
    accelerate launch --num_cpu_threads_per_process=2 train_network.py \\
      --pretrained_model_name_or_path={config['model_path']} \\
      --dataset_config={config['dataset_config']} \\
      --output_dir={config['output_dir']} \\
      --output_name={config['output_name']} \\
      --save_model_as=safetensors \\
      --prior_loss_weight=1.0 \\
      --max_train_steps={config['max_steps']} \\
      --learning_rate={config['learning_rate']} \\
      --optimizer_type="AdamW8bit" \\
      --lr_scheduler="cosine_with_restarts" \\
      --lr_warmup_steps=100 \\
      --train_batch_size=1 \\
      --gradient_checkpointing \\
      --gradient_accumulation_steps=1 \\
      --mixed_precision="fp16" \\
      --save_precision="fp16" \\
      --network_module="networks.lora" \\
      --network_rank={config['lora_rank']} \\
      --network_alpha={config['lora_alpha']} \\
      --network_train_unet_only \\
      --cache_latents \\
      --cache_latents_to_disk \\
      --persistent_data_loader_workers
    """
    
    # í•™ìŠµ ì‹¤í–‰
    os.system(train_script)
    
    # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    output_files = list(Path(config['output_dir']).glob("*.safetensors"))
    if output_files:
        return str(output_files[-1])  # ê°€ì¥ ìµœì‹  íŒŒì¼
    return None

def handler(job):
    """RunPod í•¸ë“¤ëŸ¬"""
    try:
        job_input = job["input"]
        
        # 1. ì…ë ¥ íŒŒë¼ë¯¸í„°
        dataset_url = job_input["dataset_url"]
        model_name = job_input.get("model_name", "black-forest-labs/FLUX.1-dev")
        trigger_word = job_input.get("trigger_word", "TOK")
        max_steps = job_input.get("steps", 1000)
        learning_rate = job_input.get("learning_rate", 4e-4)
        lora_rank = job_input.get("lora_rank", 32)
        
        # 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
        print(f"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ: {dataset_url}")
        dataset_path = "/tmp/dataset.zip"
        
        response = requests.get(dataset_url)
        with open(dataset_path, 'wb') as f:
            f.write(response.content)
        
        # ZIP ì••ì¶• í•´ì œ
        extract_path = "/tmp/dataset"
        os.makedirs(extract_path, exist_ok=True)
        
        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        # 3. ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ìƒì„±
        dataset_config = {
            "general": {
                "enable_bucket": True,
                "resolution": "1024,1024",
                "batch_size": 1
            },
            "datasets": [{
                "subsets": [{
                    "image_dir": extract_path,
                    "class_tokens": trigger_word,
                    "num_repeats": 10
                }]
            }]
        }
        
        # ì„¤ì • ì €ì¥
        import json
        config_path = "/tmp/dataset_config.json"
        with open(config_path, 'w') as f:
            json.dump(dataset_config, f)
        
        # 4. í•™ìŠµ ì‹¤í–‰
        print("ğŸš€ LoRA í•™ìŠµ ì‹œì‘...")
        
        train_config = {
            "model_path": model_name,
            "dataset_config": config_path,
            "output_dir": "/tmp/output",
            "output_name": f"{trigger_word}_lora",
            "max_steps": max_steps,
            "learning_rate": learning_rate,
            "lora_rank": lora_rank,
            "lora_alpha": lora_rank
        }
        
        # ì‹¤ì œ í•™ìŠµ (ê°„ë‹¨í•œ ë²„ì „)
        # ì‹¤ì œë¡œëŠ” Kohya ìŠ¤í¬ë¦½íŠ¸ë‚˜ diffusersì˜ train_text_to_image_lora.py ì‚¬ìš©
        from train_simple_lora import train_flux_lora  # ì»¤ìŠ¤í…€ í•™ìŠµ í•¨ìˆ˜
        
        output_path = train_flux_lora(
            model_name=model_name,
            train_data_dir=extract_path,
            output_dir="/tmp/output",
            instance_prompt=f"a photo of {trigger_word}",
            max_train_steps=max_steps,
            learning_rate=learning_rate,
            rank=lora_rank
        )
        
        # 5. ê²°ê³¼ ì—…ë¡œë“œ
        print("ğŸ“¤ í•™ìŠµëœ LoRA ì—…ë¡œë“œ ì¤‘...")
        
        # HuggingFace Hubì— ì—…ë¡œë“œ
        from huggingface_hub import HfApi
        api = HfApi()
        
        repo_id = f"{os.environ.get('HF_USERNAME', 'user')}/{trigger_word}-flux-lora"
        
        # ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
        api.create_repo(repo_id, exist_ok=True)
        
        # íŒŒì¼ ì—…ë¡œë“œ
        upload_url = api.upload_file(
            path_or_fileobj=output_path,
            path_in_repo=f"{trigger_word}_lora.safetensors",
            repo_id=repo_id,
            repo_type="model"
        )
        
        # ëª¨ë¸ ì¹´ë“œ ìƒì„±
        model_card = f"""
---
tags:
- flux
- lora
- diffusers
- {trigger_word}
---

# {trigger_word} FLUX LoRA

## Trigger word
`{trigger_word}`

## Usage
```python
from diffusers import FluxPipeline
import torch

pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.float16
).to("cuda")

pipe.load_lora_weights("{repo_id}")

prompt = "a photo of {trigger_word} person"
image = pipe(prompt).images[0]
```

## Training details
- Base model: {model_name}
- Steps: {max_steps}
- Learning rate: {learning_rate}
- Rank: {lora_rank}
"""
        
        api.upload_file(
            path_or_fileobj=model_card.encode(),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="model"
        )
        
        return {
            "status": "success",
            "lora_url": f"https://huggingface.co/{repo_id}",
            "trigger_word": trigger_word,
            "training_steps": max_steps,
            "download_url": f"https://huggingface.co/{repo_id}/resolve/main/{trigger_word}_lora.safetensors"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# RunPod ì„œë²„ë¦¬ìŠ¤ ì‹œì‘
runpod.serverless.start({"handler": handler})